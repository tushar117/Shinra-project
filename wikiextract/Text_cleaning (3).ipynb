{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK9ckRSk3ZQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjqHBdvMJDVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import regex as re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN9YwX261vkg",
        "colab_type": "code",
        "outputId": "177b7970-47ae-4f75-a1d2-bbe58cc38b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "def get_top_category(ene_id, ene_map):\n",
        "    target = ene_id\n",
        "    while(ene_map[target]['parent'] is not None):\n",
        "        target = ene_map[target]['parent']\n",
        "    return target\n",
        "\n",
        "def wikidump(filepath, offset=1, step=1):\n",
        "    with gzip.open(os.path.abspath(filepath), mode='rt', encoding='utf-8') as dump_file:\n",
        "        line_count=0\n",
        "        for line in dump_file:\n",
        "            line_count+=1\n",
        "            if offset > line_count or (line_count - offset)%step != 0:\n",
        "                continue\n",
        "            try:\n",
        "                yield json.loads(line.rstrip(',\\n'))\n",
        "            except json.decoder.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "def clean_text(text, re_expressions):\n",
        "    new_wiki_text=text\n",
        "    for re_name, re_string in re_expressions.items():\n",
        "        regex = re.compile(re_string, re.IGNORECASE)\n",
        "        new_wiki_text = re.sub(regex, '', new_wiki_text)\n",
        "    new_wiki_text = str(new_wiki_text)###New part added from here\n",
        "    for punct in puncts:\n",
        "        if punct in new_wiki_text:\n",
        "            new_wiki_text = new_wiki_text.replace(punct, '')\n",
        "    new_wiki_text=' '.join(new_wiki_text.split())\n",
        "    return new_wiki_text\n",
        "\n",
        "\n",
        "def get_ene_hierarchy(config):\n",
        "    raw_dicts = {}\n",
        "    with open(config.get('ene_definition_file'), 'r', encoding='utf-8') as ene_file:\n",
        "        for line in ene_file.readlines():\n",
        "            line = str(line).strip()\n",
        "            #workaround for escape characters\n",
        "            if \"\\'\" in line:\n",
        "                line = line.replace(\"\\'\", \"\\\\'\")\n",
        "            data = json.loads(line)\n",
        "            ene_id = data.get('ENE_id')\n",
        "            ene_name = data['name']['en']\n",
        "            parent_catgory = data['parent_category']\n",
        "            child_category = data['children_category']\n",
        "            raw_dicts[ene_id] = {\n",
        "                'name': ene_name,\n",
        "                'parent': parent_catgory,\n",
        "                'child': child_category,\n",
        "            }\n",
        "    return raw_dicts\n",
        "\n",
        "def get_language_data(config):\n",
        "    ene_categories = defaultdict(lambda: [])\n",
        "    print('loading language dependent data from file : %s' % (config.get('ene_lang_file')))\n",
        "    with open(config.get('ene_lang_file'), 'r', encoding='utf-8') as t_file:\n",
        "        for line in tqdm(t_file.readlines()):\n",
        "            line = str(line).strip()\n",
        "            data = json.loads(line)\n",
        "            page_id = int(str(data.get('pageid')).strip())\n",
        "            for ene_desc in data['ENEs']:\n",
        "                ene_id = ene_desc['ENE_id']\n",
        "                ene_categories[page_id].append(ene_id)\n",
        "    return ene_categories\n",
        "\n",
        "def extract_data(config):\n",
        "    dump_file_path = config.get('dumpfile')\n",
        "    ene_hierarchy = get_ene_hierarchy(config)\n",
        "    ene_pages = get_language_data(config)\n",
        "\n",
        "    page_info_start=True\n",
        "    ill_formed_data = 0\n",
        "    temp_data = {}\n",
        "    is_valid = False\n",
        "    \n",
        "    dataset_file = config.get('dataset_file')\n",
        "    if os.path.isfile(dataset_file):\n",
        "        try:\n",
        "            os.remove(dataset_file)\n",
        "        except Exception as e:\n",
        "            print('unable to delete : %s. error : %s' % (dataset_file, str(e)))\n",
        "            with open(dataset_file,'w'): pass\n",
        "    dataset_file = open(dataset_file, 'a+', encoding='utf-8')\n",
        "\n",
        "    # global_data = []\n",
        "    counter=0\n",
        "    extracted_data_count=0\n",
        "    for data in tqdm(wikidump(dump_file_path)):\n",
        "        if page_info_start:\n",
        "            counter+=1\n",
        "            temp_data = {}\n",
        "            page_data = data.get('index', None)\n",
        "            ill_formed_data+=1\n",
        "            if page_data is not None and page_data.get('_id', -1) != -1:\n",
        "                is_valid=True\n",
        "                temp_data['page_id'] = page_data.get('_id')\n",
        "                ill_formed_data-=1\n",
        "                page_info_start = False\n",
        "        else:\n",
        "            title = data.get('title', None)\n",
        "            ill_formed_data+=1\n",
        "            if title is not None and not page_info_start:\n",
        "                temp_data['title'] = title\n",
        "                page_info_start = True\n",
        "                ill_formed_data-=1\n",
        "                is_valid=False\n",
        "                curr_page_id = int(str(temp_data.get('page_id')).strip())\n",
        "                if curr_page_id in ene_pages:\n",
        "                    zero_level_classes = set()\n",
        "                    #ensuring mutiple-labels classes are included\n",
        "                    for ene_id in ene_pages[curr_page_id]:\n",
        "                        zero_level_classes.add(get_top_category(ene_id, ene_hierarchy))\n",
        "                    temp_data['classes'] = list(zero_level_classes)\n",
        "                    #loading additional fields as configures\n",
        "                    for field in config.get('fields'):\n",
        "                        if field == 'text' and config.get('use_regex'):\n",
        "                            temp_data[field] = clean_text(data['text'], config.get('exclude_regex'))\n",
        "                            temp_data[field] = clean_text(data['text'], config.get('exclude_regex'))\n",
        "                        else:\n",
        "                            temp_data[field] = data.get(field, None)\n",
        "                    #writing to file \n",
        "                    json.dump(temp_data, dataset_file, ensure_ascii=False)\n",
        "                    dataset_file.write('\\n')\n",
        "                    # global_data.append(temp_data)\n",
        "                    extracted_data_count+=1\n",
        "    #finally close the file\n",
        "    dataset_file.close()\n",
        "    print(\"count : %d, invalid counter : %d, valid counter : %d\" % (counter, ill_formed_data, counter-ill_formed_data))\n",
        "    print(\"global data extracted : %d/%d\"%(extracted_data_count, len(ene_pages)))\n",
        "\n",
        "def init():\n",
        "    # base_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "    # print(\"working in directory : \", base_dir)\n",
        "    global_config = {\n",
        "        'fields': ['text', 'category'], #fields in addition to titles and page_ids \n",
        "        'dumpfile': '/content/drive/My Drive/Shinra/hiwiki-20190121-cirrussearch-content.json.gz',\n",
        "        'dataset_file': '/content/drive/My Drive/Shinra/dataset.json',\n",
        "        'ene_definition_file': '/content/drive/My Drive/Shinra/ENE_Definition_v8.0.0.json',\n",
        "        'ene_lang_file': '/content/drive/My Drive/Shinra/hi_ENEW_LIST.json',\n",
        "        'use_regex': True,\n",
        "        # need to explore more\n",
        "        'exclude_regex': {\n",
        "            \"html_tags\": \"<\\w+[\\s|\\w]*>(?>[^<>]+|(?R))*</\\w+>\", #recursive expression don't change\n",
        "            \"url\": \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
        "            \"www\": \"www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
        "            \"english_words\": \"[a-zA-Z]+\",\n",
        "            #\"wiki_links\": \"\\[\\[(?>[^\\[\\]]+|(?R))*\\]\\]\",\n",
        "            #\"curly_braces\": \"{{(?>[^{}]+|(?R))*}}\",\n",
        "            #\"styles\" : \"\\[\\|.*?\\|\\]\",\n",
        "            #\"tables\" : '{\\|.*?\\|}',\n",
        "            #\"braces\" : '{.*?}'\n",
        "        },\n",
        "    }\n",
        "    extract_data(global_config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 13444/30547 [00:00<00:00, 134434.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading language dependent data from file : /content/drive/My Drive/Shinra/hi_ENEW_LIST.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30547/30547 [00:00<00:00, 126028.43it/s]\n",
            "264388it [02:09, 2045.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "count : 132194, invalid counter : 0, valid counter : 132194\n",
            "global data extracted : 30492/30535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbcROlUDNHqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWseuqzbMiQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Shinra/dataset.json', 'r', encoding='utf-8') as ene_file:\n",
        "    for line in ene_file.readlines():\n",
        "        line = str(line).strip()\n",
        "        #workaround for escape characters\n",
        "        # if \"\\'\" in line:\n",
        "        #     line = line.replace(\"\\'\", \"\\\\'\")\n",
        "        data.append(json.loads(line))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}